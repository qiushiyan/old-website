---
title: 'Enterprise Data Analysis and Visualization: An Interview Study'
author: Qiushi Yan
date: '2019-11-21'
slug: enterprise-data-analysis-and-visualization-an-interview-study
categories:
  - Reading Notes
tags:
  - Data Science
subtitle: ''
summary: 'Reading notes on "Enterprise Data Analysis and Visualization: An Interview Study" by Sean Kandel, Andreas Paepcke, Joseph M. Hellerstein, and Jeffrey Heer'
description: 'Reading notes on "Enterprise Data Analysis and Visualization: An Interview Study" by Sean Kandel, Andreas Paepcke, Joseph M. Hellerstein, and Jeffrey Heer'
authors: []
lastmod: '2019-11-21T18:42:03+08:00'
bibliography: bib/enterprise-data-analysis-and-visualization-an-interview-study.bib
biblio-style: "apalike"
link-citations: true
---



<p><strong>“Enterprise Data Analysis and Visualization: An Interview Study”</strong> is an effort to penetrate into enterprises and glean real-world data science experience from analysts directly. By conducting semi-structured interviews with 35 data analysts, <span class="citation">Heer (<a href="#ref-2012-enterprise-analysis-interviews" role="doc-biblioref">2012</a>)</span> provides insight on 3 archetypes describing different workflows and tasks given, common pain points analysts may encounter, future trends, etc.</p>
<div id="archetypes" class="section level1">
<h1>3 Archetypes</h1>
<p>According to the responses, we can see analysts generally fall into 3 archetypes: <em>hacker</em>, <em>scripter</em> and <em>application user</em>.<br />
<img src="/img/archetypes.png" /><!-- --></p>
<p><strong>Hackers</strong> are faced with the most diverse and complex tasks. They are most literate in terms of programming and thus rarely ask IT staffs fro help. More oftern than not, hackers master more than three languages, R / Matlab for analysis, Python, Perl as an scripting language, and SQL for queries.</p>
<p>{{% alert note %}}
I guess for now Python has gained ground also as a data analysis language, and Matlab is less popular among analysts doing “practical” analysis, but more frquently used in labs of nature science.
{{% /alert %}}</p>
<p>Also note that with their ability to collect, manipulate data, some hackers could be in charge of managing the data warehouse of the company, and dealing with large datasets. In constrast, they perform less statistical models, and spend more time in early-stage analytic activities prior to modeling, if any.</p>
<p><strong>Scripters</strong> take care of advanced modeling and use a software package such as R or Matlab extensively. They are less proficient when parsing log files or scraping data off the web, and the data susceptible for modeling are often prepared by IT staff.</p>
<p><strong>Application user</strong> prefer operations done in a spreadsheet( Mostly in Excel ), or other analysis application (e.g., SAS / JMP, SPSS, etc.). Data are also pulled out from several relationald databases prior to their work. They typically worked on smaller datasets than other gorups, advanced application users may wrote scripts using an embedded language such as Visual Basic.</p>
<p>{{% alert note %}}
To my mind, application users dipicted here seem to be titled “data analysts” for historical reasons, and there is little case for keeping such a position when there are already hackers and scripters. Perhaps they were traditional bussiness people in charge of analysis or accountants, so they are most comforatble with Microsoft Excel or SPSS. As the “big data” meme started to present itself, all of a sudden their enterprise felr it imperative to set up a “data scientist / analyst” position to keep up with this new trend. Yet I doubt if this archetype could still survive when graduates from data.
{{% /alert %}}</p>
</div>
<div id="analysts-within-organization" class="section level1">
<h1>Analysts within organization</h1>
<p>Analysts interact closely with IT staff to complete aspects of theire job. For IT staff, his relationship includes data ingesting and acquiring, operationalizing recurring workflows, and serve as a source of documentation when analysts, say, con’t figure out how to wirte a complex SQL statement.</p>
<p>This reliance on IT staff was particularly true in organizations where data was distributed across many different data sources. Hackers were most likely to use the IT team explicitly for this function, as they were more likely to access data directly from the warehouse. Scripters and application users relied on this function implicitly when receiving data from members of IT.</p>
<p>Another thread of this topic is distributed data, which are generated from multiple departments of the enterprise. They are often stored in various databases and formats. adding to the diffculty of intergrating them.</p>
<p>When analysis is finished, analysts typically shared static reports in the form of template documents. In some cases, reports could be interactive dashboards that enabled end users to filter or modify statistics computed. It’s not hard to imagine consumers of the report often give a blurred image of what they want, and hardly could analysts translate them into practical data problems.</p>
<p>When it comes to collaboration, it is an exception rather than the rule for analysts. They do share some central repository data processing scriptes are kept to oneself as a rule. Ont the other hand, final reports in the form of charts or model summary are commonly shared among analytsts in planning meeting or presentations. These reports, however, are rarely parametrizable or interactive.</p>
<p>The author identifies three impediemnts to collaboration:</p>
<ul>
<li>the diversity of tools and programming languages<br />
</li>
<li>finding a script or inter-
mediate data product someone else produced was often more time-
consumingthanwritingthescriptfromscratch<br />
</li>
<li>many analysis process are “ad hoc”, “experimental”</li>
</ul>
</div>
<div id="challenges" class="section level1">
<h1>Challenges</h1>
<p>Five high-level tasks in the workflow are proposed and and challenges within each of them elaborated.</p>
<p><strong>Discovering</strong><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>: Since data are often distributed across multiple databases and sources, finding the exact data sheet or file needed can be time-consuming, not to mention that some analysts only have restricted access to the data warehhouse. Another problem is field definitions, these definitions were
often missing in relational databases and non-existent in other types of data stores.</p>
<p><strong>Wranling</strong>: Many analysts reported parsing, ingesting semi-structured data(i.e., log files, block data<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>). Another difficulty is integrating the data, after analysts manage to find them in databases. Sometimes identifiers are missing or encoded inconsistantly in some databases, and sometimes there is no column than could be an identifier.</p>
<p><strong>Profiling</strong><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>: Many analysts (22 / 35) reports issues dealing with missing data and heterogeneous data in a column. When detecting outliers, there is no general agreement between visualization and traditional staistical methods.</p>
<p><strong>Modeling</strong>: The biggest challenge in constructing an model according to respondents are feature selection, whether to choose a set of variables, which to transfrom and how to transform.</p>
<p>{{% alert note %}}
There is a great book on feature engineering and selectin by Max Kuhn and Kjell Johnson: <a href="http://www.feat.engineering/index.html">Feature Engineering and Selection: A Practical Approach for Predictive Models</a>
{{% /alert %}}</p>
<p>Most repondents also pointed to the scalibility of existing analysis and visualization tools. Hackers are less limited by large amounts of data, obviously, but hackers were often limited by the types of analysis they could run because useful models or algorithms did not have available parallelized implementations. Visualizing model results is another pain point, analysts using more advanced machine learning methods (14/35) expressed a desire for visualization tools to help explore these models and visualize their output.</p>
<p>{{% alert note %}}
R’s package <strong>broom</strong> <span class="citation">(Robinson and Hayes <a href="#ref-R-broom" role="doc-biblioref">2019</a>)</span> are desgined to facilitate modeling diagnosis, visualization, etc. <strong>tidymodels</strong> <span class="citation">(Kuhn and Wickham <a href="#ref-R-tidymodels" role="doc-biblioref">2019</a>)</span> contains a burgeoning list of such packages.
{{% /alert %}}</p>
<p><strong>Reporing</strong>: The two most-cited challenges in reporoting were communicating assumptions and building interactive reports. Documentation that should have been provided alongside the report are often missing or poorly written. Even when assumptions were tracked, they were often treated as footnotes instead of first-class results. Moreover, analysts complained that reports were too
inflexible and did not allow interactive verification or sensitivity analysis.</p>
</div>
<div id="future-trends" class="section level1">
<h1>Future trends</h1>
<p>The article then prophesy future trends in technology and the analytic workforce:</p>
<p>Public data become more accessible and add to the diffculty of data discovery and intergration.</p>
<p>The market of Hadoop-like software will continue to increase, allowing analysts to operate on less structured data formats.</p>
<p>“Hacker-level” analysts will be in demand, analysts therefore need to be adept at both statistical reasoning and writing complex SQL or Map-Reduce code. Those who are comfortable in multiple data processing / analysis frameworks will be competent.</p>
<p>The size of analytic teams should grow, efficient collaboration will become both increasingly important and difficult.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<div id="refs" class="references">
<div id="ref-R-rmarkdown">
<p>Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2019. <em>Rmarkdown: Dynamic Documents for R</em>. <a href="https://CRAN.R-project.org/package=rmarkdown">https://CRAN.R-project.org/package=rmarkdown</a>.</p>
</div>
<div id="ref-R-shiny">
<p>Chang, Winston, Joe Cheng, JJ Allaire, Yihui Xie, and Jonathan McPherson. 2019. <em>Shiny: Web Application Framework for R</em>. <a href="https://CRAN.R-project.org/package=shiny">https://CRAN.R-project.org/package=shiny</a>.</p>
</div>
<div id="ref-2012-enterprise-analysis-interviews">
<p>Heer, Sean Kandel AND Andreas Paepcke AND Joseph Hellerstein AND Jeffrey. 2012. “Enterprise Data Analysis and Visualization: An Interview Study.” In <em>IEEE Visual Analytics Science &amp; Technology (Vast)</em>. <a href="http://idl.cs.washington.edu/papers/enterprise-analysis-interviews">http://idl.cs.washington.edu/papers/enterprise-analysis-interviews</a>.</p>
</div>
<div id="ref-R-tidymodels">
<p>Kuhn, Max, and Hadley Wickham. 2019. <em>Tidymodels: Easily Install and Load the ’Tidymodels’ Packages</em>. <a href="https://CRAN.R-project.org/package=tidymodels">https://CRAN.R-project.org/package=tidymodels</a>.</p>
</div>
<div id="ref-R-sparklyr">
<p>Luraschi, Javier, Kevin Kuo, Kevin Ushey, JJ Allaire, and The Apache Software Foundation. 2019. <em>Sparklyr: R Interface to Apache Spark</em>. <a href="https://CRAN.R-project.org/package=sparklyr">https://CRAN.R-project.org/package=sparklyr</a>.</p>
</div>
<div id="ref-R-broom">
<p>Robinson, David, and Alex Hayes. 2019. <em>Broom: Convert Statistical Analysis Objects into Tidy Tibbles</em>. <a href="https://CRAN.R-project.org/package=broom">https://CRAN.R-project.org/package=broom</a>.</p>
</div>
<div id="ref-R-tidyverse">
<p>Wickham, Hadley. 2017. <em>Tidyverse: Easily Install and Load the ’Tidyverse’</em>. <a href="https://CRAN.R-project.org/package=tidyverse">https://CRAN.R-project.org/package=tidyverse</a>.</p>
</div>
<div id="ref-R-knitr">
<p>Xie, Yihui. 2019. <em>Knitr: A General-Purpose Package for Dynamic Report Generation in R</em>. <a href="https://CRAN.R-project.org/package=knitr">https://CRAN.R-project.org/package=knitr</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Here discovering means acquire data necessary to complete analysis tasks<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>In a block format, logical records of data are spread
across multiple lines of a file. Typically one line (the “header”) con-
tains metadata about the record, such as how many of the subsequent
lines (the “payload”) belong to the record<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>To verify data qualityand its suitability for the analysis tasks. Example tasks include inspecting outliers, examining distributions<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>In the last section, the author move on to discuss improvements that can be made to analytic tools. I figure these suggestions are somehow outdated by today’s standard, and <code>tidyverse</code><span class="citation">(Wickham <a href="#ref-R-tidyverse" role="doc-biblioref">2017</a>)</span>、<code>knitr</code><span class="citation">(Xie <a href="#ref-R-knitr" role="doc-biblioref">2019</a>)</span>、<code>rmarkdown</code><span class="citation">(Allaire et al. <a href="#ref-R-rmarkdown" role="doc-biblioref">2019</a>)</span>、<code>shiny</code><span class="citation">(Chang et al. <a href="#ref-R-shiny" role="doc-biblioref">2019</a>)</span>、<code>sparklyr</code><span class="citation">(Luraschi et al. <a href="#ref-R-sparklyr" role="doc-biblioref">2019</a>)</span> and some other packages in R have already given fine solutions to them, so this section is skipped.<a href="#fnref4" class="footnote-back">↩</a></p></li>
</ol>
</div>
