---
title: Tidy Data with Python
author: Qiushi Yan
date: '2020-05-12'
slug: python-tidy-data
categories:
  - Python
  - R
  - Data Analysis
summary: "A play with messy data in Hadley Wickham's Tidy Data paper in pandas, finish by exploring a real-world dataset using both R and Python."
subtitle: "A play with messy data in Hadley Wickham's Tidy Data paper in pandas, finish by exploring a real-world dataset using both R and Python."
lastmod: '2020-05-12T22:11:05+08:00'
bibliography: ../bib/python-tidy-data.bib
biblio-style: apalike
link-citations: yes
---

This week I've been doing some recap on how to do basic data processing and cleaning in Python with the `pandas` and `NumPy` library. So this post is mostly a self-reminder on how to deal with messy data in Python, by reproducing data cleaning examples presented in Hadley Wickham's [Tidy Data](https://vita.had.co.nz/papers/tidy-data.pdf) paper, @JSSv059i10. 


The most significant contribution of this well-known work is that it gave clear definition on what "tidy" means for a dataset. There are 3 main requirements, as illustrated on [`tidyr`](https://tidyr.tidyverse.org/)'s website (evolving from what Hadley originally proposed): 

1. Every column is a variable. 
2. Every row is an observation.
3. Every cell is a single value. 

Messy data are, by extension, datasets in volation of these 3 rules. The author then described the five most common problems with messy datasets:  

- Column headers are values, not variable names.
- Multiple variables are stored in one column.  
- Variables are stored in both rows and columns.
- Multiple types of observational units are stored in the same table.  
- A single observational unit is stored in multiple tables.  

In this post I will be focusing on  the first 3 symptoms since the other two violations often occur when working with databases. All datasets come from Hadley's [repo](https://github.com/hadley/tidy-data) containing materials for the paper and [Daniel Chen](https://chendaniely.github.io/)'s 2019 SciPy tutorial on data processing with pandas.

```{r, include = FALSE}
knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      comment = "#>",
                      collapse = TRUE)
```


```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```

```{python, echo = FALSE}
import os
os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'D:/Anaconda/Library/plugins/platforms'
```


```{r, echo = FALSE}
library(tidyverse)
library(reticulate)

theme_bar <- function(base_size = 11,
                      strip_text_size = 12,
                      strip_text_margin = 5,
                      subtitle_size = 13,
                      subtitle_margin = 10,
                      plot_title_size = 16,
                      plot_title_margin = 10,
                      ...) {
  ret <- ggplot2::theme_minimal(base_size = base_size, ...)
  ret$strip.text <- ggplot2::element_text(
    hjust = 0, size = strip_text_size,
    margin = ggplot2::margin(b = strip_text_margin)
  )
  ret$plot.subtitle <- ggplot2::element_text(
    hjust = 0, size = subtitle_size,
    margin = ggplot2::margin(b = subtitle_margin)
  )
  ret$plot.title <- ggplot2::element_text(
    hjust = 0, size = plot_title_size,
    margin = ggplot2::margin(b = plot_title_margin)
  )
  ret
}

ggplot2::theme_set(theme_bar())
```


# Column headers are values, not variable names  

```{python}
pew = pd.read_csv("D:/RProjects/data/blog/pew.csv")
pew.head()
```

The `pew` dataset explores the relationship between income and religion in the US, produced by the Pew Research Center. To tidy it, we think of its "right" form if we are to answer data analysis questions. Say, what if we want to a person's income is influenced by his religion or the other way around. It should be obvious that we need to derive from `pew` a column to indicate the level of a person's income, and another column being count of any combination of income and religion. `pew` is messy in the sense that all column names besides `religion`, from `<$10k` to `Don't know/refused`, should be different levels (values) of a column storing information about income.

The code to do this is fairly easy in pandas, the `.melt` method is very similar to `tidyr::pivot_longer` and its namesake in the retired `reshape2` package.  
```{python}
tidy_pew = pew.melt(id_vars = "religion", var_name = "income", value_name = "count")
tidy_pew.head(20)
```


Now let's calculate the $\chi^2$ statistic in R and the corresponding p value；
```{r}
# R 
library(infer)
chisq_stat <- py$tidy_pew %>%
  tidyr::uncount(count) %>% 
  specify(religion ~ income) %>% 
  hypothesize(null = "independence") %>%
  calculate(stat = "Chisq")

py$tidy_pew %>%
  tidyr::uncount(count) %>% 
  specify(religion ~ income) %>% 
  hypothesize(null = "independence") %>%
  visualize(method = "theoretical") +
  shade_p_value(obs_stat = chisq_stat, direction = "right")
```

This shows strong relationship between `income` and `religion`. 

Another common use of this wide data format is to record regularly spaced observations over time, illustrated by the `billboard` dataset. Ther rank of a specific track in each week after it enters the Billboard top 100 is recorded in 75 columns, `wk1` to `wk75`.

```{python}
billboard = pd.read_csv("D:/RProjects/data/blog/billboard.csv")
billboard
```

If we are to answer questions like "what are the average ranking of artisits across all weeks?", `wk` like columns need to be transformed into values:  ^[Although pandas and dplyr 1.0 can perform rowwise operatios in a breeze, it's not considered best practice in such cases.]

```{python}
tidy_billboard = billboard.melt(id_vars = ["year", "artist", "track", "time", "date.entered"],
                                var_name = "week",
                                value_name = "rank")
                                
tidy_billboard
```

Now we can compute the average ranking:  

```{python}
(tidy_billboard.
  groupby("artist")[["rank"]].
  mean().
  sort_values(by = "rank")
)
```

# Multiple variables stored in one column


> The `tb` daaset comes from the World Health Organisation, and records the counts of confirmed tuberculosis cases by country, year, and demographic group. The demographic groups are broken down by sex (m, f)  and age (0–14,  15–25,  25–34,  35–44.  

```{python}
tb = pd.read_csv("D:/RProjects/data/blog/tb.csv")
tb
```


To clean this data, we first melt all columns except for `country` and `year` in return for a longer version of `tb`, and then seperate the `variable` column into two pieces of information, `sex` and `age`.

```{python}
tb_long = tb.melt(id_vars = ["country", "year"])
sex = tb_long["variable"].str.split(pat = "(m|f)(.+)").str.get(1)
age = tb_long["variable"].str.split(pat = "(m|f)(.+)").str.get(2)

print(sex)
print(age)
```

Add these two columns and drop the redundant `variable` column

```{python}
tidy_tb = tb_long.assign(sex = sex, age = age).drop("variable", axis = "columns")
tidy_tb
```



# Variables are stored in both rows and columns

> The `weather` data shows daily weather data from the Global Historical Climatology Network for one  weather station (MX17004) in Mexico for five months in 2010. 

```{python}
weather = pd.read_csv("D:/RProjects/data/blog/weather.csv")
weather
```

There are two major problems with `weather`:  

- `d1`, `d2`, ..., `d31` should be values instead of column names (solved by `.melt`) 
- On the other hand, values in the `element` column should be names, it should be spread into two columns named `tmax`, `tmin` (solved by `.pivot_table`)

```{python}
(weather.
  melt(id_vars = ["id", "year", "month", "element"], var_name = "day", value_name = "temp").
  pivot_table(index = ["id", "year", "month", "day"],
              columns = "element",
              values = "temp").
  reset_index().
  head()
)
```


# Case study: mortality data from Mexico  


After stating these common problems and their remidies, Hadley presented a case study section on how tidy dataset can facilitate data analysis. The case study uses individual-level mortality data from Mexico.  The goal is to find causes of death with unusual temporal patterns, at hour level. It's time to move back from Python to R!

```{r}
library(tidyverse)
deaths <- read_csv("D:/RProjects/data/blog/mexico-deaths.csv") %>% na.omit()
deaths
```

The columns are year, month, day, hour and cause of specific death respectively. Another table `codes` explains what acronyms in `cod` mean. 

```{r}
codes <- read_csv("D:/RProjects/data/blog/codes.csv")
codes
```

Thanks to the [`reticulate`](https://rstudio.github.io/reticulate/) package, we can mix R and Python code seamlessly. Here is a line plot made with `seaborn` demonstrating total deaths per hour:


```{python}
# Python
deaths = r.deaths
deaths_per_hour = deaths["hod"].value_counts()
deaths_per_hour
sns.lineplot(x = deaths_per_hour.index, y = deaths_per_hour.values)
plt.title("Temporal pattern of all causes of death")
plt.xlabel("Hour of the day")
plt.ylabel("Number of deaths")
plt.show()
```

To provide informative labels for causes, we next join the dataset to the `codes` dataset, on the `cod` variable. 


```{r}
deaths <- left_join(deaths, codes) %>%
  rename(cause = disease)
head(deaths)
```


The total deaths for each cause varies over several orders of magnitude: there are 46,794 deaths from heart attack but only 1 from Tularemia.  
```{python}
# Python
(deaths.groupby(["cod"]).
  size().
  reset_index(name = "per_cause").
  sort_values(by = "per_cause", ascending = False)
)
```

This means that rather than the total number, it makes more sense to think in proportions. If a cause of death departs from the overall temporal pattern, then its proportion of deaths in a given hour compared to the total deaths of that cause should differ significantly from that of the hourly deaths at the same time compared to total deaths. I denote these two proportions as `prop1` and `prop2` respectively. To ensure that the causes we consider are sufficiently representative we’ll only work with causes with more than 50 total deaths.

```{r}
prop1 <- deaths %>% 
  count(hod, cause, name = "per_hour_per_cause") %>% 
  add_count(cause, wt = per_hour_per_cause, name = "per_cause") %>% 
  mutate(prop1 = per_hour_per_cause / per_cause)

prop2 <- deaths %>% 
  count(hod, name = "per_hour") %>% 
  add_count(wt = per_hour, name = "total") %>% 
  mutate(prop2 = per_hour / total)
```

Hadley used mean square error between the two proportions as a kind of distance, to indicate the average degree of anomaly of a cause, and I follow:  

```{r}
dist <- prop1 %>% 
  filter(per_cause > 50) %>% 
  left_join(prop2, on = "hod") %>% 
  select(hour = hod,
         cause,
         n = per_cause,
         prop1,
         prop2) %>% 
  group_by(cause, n) %>% 
  summarize(dist = mean((prop1 - prop2) ^ 2)) %>% 
  ungroup()

dist %>% 
  arrange(desc(dist))
```

Here we see causes of death with highest `dist` are mainly accidents and rare diseases. However, there is a negative correlation between the frequency of a cause and its deviation, as shown in the following plot, so that the result based solely on the `dist` column would be biased in favour of rare causes.


```{r, fig.height = 10, fig.width = 10}
dist %>% 
  ggplot(aes(n, dist)) + 
  geom_jitter() + 
  ggrepel::geom_text_repel(aes(label = cause),
                           top_n(dist, 10)) + 
  scale_x_log10() + 
  scale_y_log10() + 
  geom_smooth(method = "lm") + 
  labs(title = "Temporal deviation of causes of deaths in Mexico",
       y = NULL,
       x = "total death")
```


Thus, our final solution is to build a model with `n` as predictor, and `dist` as response. The cause with highest residual are assumed to have the most deviation. Since the linear trend fits the data quite well, I opt for linear regression (Hadley used robust linear model in the paper). 

```{r}
library(broom)
lm_fit <- lm(log(dist) ~ log(n), data = dist)
tidy(lm_fit)
```

Let's plot these residuals against the predictor `log(n)`:

```{r}
augment(lm_fit) %>% 
  ggplot(aes(log.n., .resid)) + 
  geom_hline(yintercept = 0, color = "red") + 
  geom_point()
```


> The plot shows an empty region around a residual of 1.5.  So somewhat arbitrarily, we’ll select those diseases with a residual greater than 1.5  

```{r}
rows <- augment(lm_fit) %>% 
  mutate(row = row_number()) %>% 
  filter(.resid > 1.5) %>% 
  select(row) %>% 
  pull(row)


unusual <- dist %>%
  mutate(row = row_number()) %>% 
  filter(row %in% rows) %>% 
  select(cause, n)

unusual
```

Finally, we plot the temporal course for each unusual cause of death.  

```{r, fig.height = 18, fig.width = 16}
prop1 %>% 
  filter(cause %in% unusual$cause) %>% 
  left_join(prop2, on = "hod") %>% 
  pivot_longer(c(prop1, prop2)) %>% 
  ggplot(aes(hod, value, color = name)) + 
  geom_line() + 
  scale_color_manual(name = NULL,
                     labels = c("cause-specific", "overall"), 
                     values = c("#FFBF0F", "#0382E5")) + 
  facet_wrap(~ cause, scales = "free_y") + 
  labs(x = "hour", y = NULL, title = "Most deviated causes of death", 
       subtitle = "comparing cause-specific temporal pattern to overall trend") + 
  theme_minimal(base_size = 22) +
  theme(legend.position = "top") 
```







# References  